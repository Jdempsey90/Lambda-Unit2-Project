{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anime Watching Status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set that I am using for this project is the *MyAnimeList Database 2020* from Kaggle.com\n",
    "\n",
    "```\n",
    "https://www.kaggle.com/hernan4444/anime-recommendation-database-2020?select=animelist.csv\n",
    "```\n",
    "\n",
    "Using this dataset, I will be attempting to predict the watching status of a given user.  There are 6 different watching statuses, making this a classification problem.  The reason I am choosing watching status as a target is that watching status was easier to isolate and reduce the risk of data leakage.  To further limit the risk of Data leakage, I removed features that might suggest a users opinion of an anime series/movie--with the exception of episodes watched.\n",
    "\n",
    "The baseline I am using is the max value of the normalized value counts in watching status; or 0.601.\n",
    "\n",
    "The metric I have chosen to measure the performance of my models is accuracy.  Although, given the distribution of the categories in watching status, it would give a better picture to use a confusion plot instead.  The reason I have chosen to stick to accuracy, is that it is the most functional metric for the end use.\n",
    "\n",
    "The best use case for this model would be for general entertainment.  Though prediction of watching status may have better uses, I believe it would not accurately represent the data without using the rest of the dataset--which would require a more advanced approach, such as nlp and market segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "## sklearn ##\n",
    "from sklearn.model_selection import train_test_split,\\\n",
    "                                    RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Category Encoders\n",
    "from category_encoders import OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wrangle(anime_filepath, animelist_filepath=None):\n",
    "    '''Takes anime.csv path, and animelist.csv path; merges and wrangles them'''\n",
    "    \n",
    "    def search_and_return(feature_column, search_pattern):\n",
    "        '''Takes a pandas Series and and search parttern.  Iterates over \n",
    "        Series and searches pattern with re.search(), concatinating search \n",
    "        results to a list.  This list is returned.'''\n",
    "    \n",
    "        temp_list = []\n",
    "        \n",
    "        for i in feature_column:\n",
    "            \n",
    "            match = re.findall(search_pattern, str(i))\n",
    "            \n",
    "            if match:\n",
    "                temp_list += [match[0]]\n",
    "            else:\n",
    "                temp_list += [np.NaN]\n",
    "                \n",
    "        return temp_list\n",
    "    \n",
    "    \n",
    "    ## Anime ##\n",
    "    \n",
    "    # dictionary of explicit data types for the anime_df dataframe as read in from anime.csv\n",
    "    anime_dtypes_dict = {\n",
    "        'MAL_ID': 'int64',\n",
    "        'Name': 'object',\n",
    "        'Score': 'float16',\n",
    "        'Genres': 'object',\n",
    "        'English name': 'object',\n",
    "        'Japanese name': 'object',\n",
    "        'Type': 'category',\n",
    "        'Episodes': 'float16',\n",
    "        'Aired': 'object',\n",
    "        'Premiered': 'object',\n",
    "        'Producers': 'object',\n",
    "        'Licensors': 'object',\n",
    "        'Studios': 'object',\n",
    "        'Source': 'object',\n",
    "        'Duration': 'object',\n",
    "        'Rating': 'object',\n",
    "        'Ranked': 'float32',\n",
    "        'Popularity': 'float32',\n",
    "        'Members': 'float32',\n",
    "        'Favorites': 'float32',\n",
    "        'Watching': 'float32',\n",
    "        'Completed': 'float32',\n",
    "        'On-Hold': 'float32',\n",
    "        'Dropped': 'float32',\n",
    "        'Plan to Watch': 'float32',\n",
    "        'Score-10': 'float32',\n",
    "        'Score-9': 'float32',\n",
    "        'Score-8': 'float32',\n",
    "        'Score-7': 'float32',\n",
    "        'Score-6': 'float32',\n",
    "        'Score-5': 'float32',\n",
    "        'Score-4': 'float32',\n",
    "        'Score-3': 'float32',\n",
    "        'Score-2': 'float32',\n",
    "        'Score-1': 'float32'\n",
    "        }\n",
    "    \n",
    "    # reading anime.csv into anime_df\n",
    "    anime_df = pd.read_csv(anime_filepath, na_values='Unknown', dtype=anime_dtypes_dict)\n",
    "\n",
    "    # drop columns from anime_df, first pass\n",
    "    anime_df = anime_df.drop(columns=[\n",
    "        'Name', \n",
    "        'Japanese name',\n",
    "        'Ranked',\n",
    "        'Popularity',\n",
    "        'Watching',\n",
    "        'Completed',\n",
    "        'On-Hold',\n",
    "        'Dropped',\n",
    "        'Plan to Watch',\n",
    "        'Score-10', \n",
    "        'Score-9', \n",
    "        'Score-8', \n",
    "        'Score-7', \n",
    "        'Score-6', \n",
    "        'Score-5', \n",
    "        'Score-4', \n",
    "        'Score-3', \n",
    "        'Score-2', \n",
    "        'Score-1',\n",
    "        'Aired'\n",
    "        ])\n",
    "\n",
    "\n",
    "    # removing adult/explicit anime titles\n",
    "    # iterate through anime_df['Genres'] and return True for all titles who's genre does not contain \n",
    "    # the word 'hentai', then subset the anime_df dataframe with only 'True'/'safe' titles.\n",
    "    no_hentai_mask = [False if re.search(r'[Hh]entai', str(i)) else True for i in anime_df['Genres']]\n",
    "    anime_df = anime_df[no_hentai_mask]\n",
    "    \n",
    "    # renaming columns\n",
    "    anime_df = anime_df.rename(columns={\n",
    "        'MAL_ID': 'anime_id',\n",
    "        'Score': 'avg_score',\n",
    "        'English name': 'english_name',\n",
    "        'Type': 'media_type',\n",
    "        'Episodes': 'total_episodes',\n",
    "        'Premiered': 'premier_date',\n",
    "        'Producers': 'producers',\n",
    "        'Licensors': 'licensors',\n",
    "        'Studios': 'studios',\n",
    "        'Source': 'source_material',\n",
    "        'Duration': 'episode_duration',\n",
    "        'Rating': 'age_rating',\n",
    "        'Members': 'groups_member_count',\n",
    "        'Favorites': 'favorited_by_users'\n",
    "        })\n",
    "    \n",
    "    \n",
    "    ## Anime List ##\n",
    "    \n",
    "    # dictionary of explicit data types for the animelist_df dataframe as read in from animelist.csv\n",
    "    animelist_dtypes_list = {\n",
    "        'user_id': 'int32',\n",
    "        'anime_id': 'int32',\n",
    "        'rating': 'int8',\n",
    "        'watching_status': 'int8',\n",
    "        'watched_episodes': 'int32'\n",
    "        }\n",
    "    \n",
    "    # reading animelist.csv into animelist_df\n",
    "    animelist_df = pd.read_csv(animelist_filepath, dtype=animelist_dtypes_list)\n",
    "    \n",
    "    \n",
    "    # sample to reduce size so it is easier on RAM\n",
    "    animelist_df = animelist_df.sample(frac=0.01, random_state=42)\n",
    "    \n",
    "    # sort both dataframes and merge them\n",
    "    anime_df = anime_df.sort_values(by='anime_id')\n",
    "    animelist_df = animelist_df.sort_values(by='anime_id')\n",
    "    anime = animelist_df.merge(anime_df, how='outer', on='anime_id')\n",
    "    \n",
    "    \n",
    "    ## Feature Engineering ##\n",
    "    anime['premier_year'] = search_and_return(anime['premier_date'], r'\\d\\d\\d\\d')\n",
    "    anime['premier_season'] = search_and_return(anime['premier_date'], r'(.+?)\\s')\n",
    "    anime['episode_minutes'] = search_and_return(anime['episode_duration'], r'\\d\\d?\\d?')\n",
    "   \n",
    "    \n",
    "    # final round of column dropping (dropping columns no longer needed after wrangling)\n",
    "    anime = anime.drop(columns=[\n",
    "        'anime_id', \n",
    "        'user_id', \n",
    "        'producers',\n",
    "        'licensors',\n",
    "        'studios',\n",
    "        'premier_date',\n",
    "        'english_name',\n",
    "        'age_rating',\n",
    "        'episode_duration',\n",
    "        'Genres',\n",
    "        'rating'\n",
    "        ])\n",
    "    \n",
    "    # drop all NaNs\n",
    "    anime = anime.dropna()\n",
    "    \n",
    "    # type casting\n",
    "    anime['premier_year'] = anime['premier_year'].astype(np.int16)\n",
    "    anime['episode_minutes'] = anime['episode_minutes'].astype(np.int16)\n",
    "    \n",
    "    \n",
    "    return anime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set File paths for csv files\n",
    "anime_filepath = r'C:\\Users\\DmgProne\\Desktop\\VSCode\\Lambda\\Unit 2\\Sprint 3\\Anime Dataset\\anime.csv'\n",
    "animelist_filepath = r'C:\\Users\\DmgProne\\Desktop\\VSCode\\Lambda\\Unit 2\\Sprint 3\\Anime Dataset\\animelist.csv'\n",
    "\n",
    "# import and wrangle data with wrangle function\n",
    "anime = wrangle(anime_filepath, animelist_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>watching_status</th>\n",
       "      <th>watched_episodes</th>\n",
       "      <th>avg_score</th>\n",
       "      <th>media_type</th>\n",
       "      <th>total_episodes</th>\n",
       "      <th>source_material</th>\n",
       "      <th>groups_member_count</th>\n",
       "      <th>favorited_by_users</th>\n",
       "      <th>premier_year</th>\n",
       "      <th>premier_season</th>\n",
       "      <th>episode_minutes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>654212</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.648438</td>\n",
       "      <td>TV</td>\n",
       "      <td>20.0</td>\n",
       "      <td>Manga</td>\n",
       "      <td>857824.0</td>\n",
       "      <td>12713.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>Winter</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342481</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.089844</td>\n",
       "      <td>TV</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Novel</td>\n",
       "      <td>2616.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1975</td>\n",
       "      <td>Spring</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631246</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.699219</td>\n",
       "      <td>TV</td>\n",
       "      <td>52.0</td>\n",
       "      <td>Original</td>\n",
       "      <td>2557.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>Spring</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016431</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.519531</td>\n",
       "      <td>TV</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Original</td>\n",
       "      <td>317337.0</td>\n",
       "      <td>2119.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>Fall</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973179</th>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>6.480469</td>\n",
       "      <td>TV</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Manga</td>\n",
       "      <td>152026.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>2018</td>\n",
       "      <td>Spring</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607187</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.351562</td>\n",
       "      <td>TV</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Light novel</td>\n",
       "      <td>190201.0</td>\n",
       "      <td>879.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>Summer</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706937</th>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.808594</td>\n",
       "      <td>TV</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Manga</td>\n",
       "      <td>1895488.0</td>\n",
       "      <td>45519.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>Summer</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728338</th>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.890625</td>\n",
       "      <td>TV</td>\n",
       "      <td>24.0</td>\n",
       "      <td>Manga</td>\n",
       "      <td>1394358.0</td>\n",
       "      <td>22304.0</td>\n",
       "      <td>2014</td>\n",
       "      <td>Fall</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058641</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.828125</td>\n",
       "      <td>TV</td>\n",
       "      <td>12.0</td>\n",
       "      <td>Manga</td>\n",
       "      <td>17915.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>2020</td>\n",
       "      <td>Summer</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948996</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.929688</td>\n",
       "      <td>TV</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Light novel</td>\n",
       "      <td>270878.0</td>\n",
       "      <td>6113.0</td>\n",
       "      <td>2017</td>\n",
       "      <td>Summer</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         watching_status  watched_episodes  avg_score media_type  \\\n",
       "654212               6.0               0.0   7.648438         TV   \n",
       "342481               3.0               1.0   7.089844         TV   \n",
       "631246               4.0               1.0   6.699219         TV   \n",
       "1016431              6.0               0.0   7.519531         TV   \n",
       "973179               2.0              12.0   6.480469         TV   \n",
       "607187               4.0               0.0   7.351562         TV   \n",
       "706937               2.0              12.0   7.808594         TV   \n",
       "728338               2.0              24.0   7.890625         TV   \n",
       "1058641              6.0               0.0   6.828125         TV   \n",
       "948996               2.0               7.0   8.929688         TV   \n",
       "\n",
       "         total_episodes source_material  groups_member_count  \\\n",
       "654212             20.0           Manga             857824.0   \n",
       "342481             26.0           Novel               2616.0   \n",
       "631246             52.0        Original               2557.0   \n",
       "1016431            12.0        Original             317337.0   \n",
       "973179             12.0           Manga             152026.0   \n",
       "607187             12.0     Light novel             190201.0   \n",
       "706937             12.0           Manga            1895488.0   \n",
       "728338             24.0           Manga            1394358.0   \n",
       "1058641            12.0           Manga              17915.0   \n",
       "948996              7.0     Light novel             270878.0   \n",
       "\n",
       "         favorited_by_users  premier_year premier_season  episode_minutes  \n",
       "654212              12713.0          2014         Winter               24  \n",
       "342481                 18.0          1975         Spring               24  \n",
       "631246                 16.0          2013         Spring               24  \n",
       "1016431              2119.0          2018           Fall               23  \n",
       "973179                844.0          2018         Spring               24  \n",
       "607187                879.0          2013         Summer               23  \n",
       "706937              45519.0          2014         Summer               24  \n",
       "728338              22304.0          2014           Fall               24  \n",
       "1058641                36.0          2020         Summer               23  \n",
       "948996               6113.0          2017         Summer               22  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display sample of the dataset to help \n",
    "# visualize the structure of the data\n",
    "anime.sample(frac=0.001).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Val, and Test Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y where the target is watching_status\n",
    "X = anime.drop(columns='watching_status')\n",
    "y = anime['watching_status']\n",
    "\n",
    "# create Train, Validation, and Test sets (70:15:15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.3)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X, y, train_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################################################################\n",
      "\n",
      "Baseline Accuracy:  0.6009775041797615\n",
      "\n",
      "###################################################################\n"
     ]
    }
   ],
   "source": [
    "# Getting and reporting Baseline Accuracy\n",
    "baseline_acc = anime['watching_status'].value_counts(normalize=True).max()\n",
    "\n",
    "print('\\n###################################################################\\n')\n",
    "print('Baseline Accuracy: ', baseline_acc)\n",
    "print('\\n###################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################################################################\n",
      "\n",
      "Logistic Regression Accuracy Scores:\n",
      "Train Score: 0.6022238115157812\n",
      "Val Score: 0.6002995668425385\n",
      "\n",
      "###################################################################\n"
     ]
    }
   ],
   "source": [
    "# defining parameter distribution for RandomSearchCV\n",
    "log_params = {\n",
    "    'C': np.arange(0.1, 1.0, 0.1),\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# establishing pipeline\n",
    "# randomize search is set to take only 3 samples to save on time, but \n",
    "# would ideally be increased to find better parameters\n",
    "model_log = make_pipeline(\n",
    "    OrdinalEncoder(),\n",
    "    RandomizedSearchCV(\n",
    "        LogisticRegression(\n",
    "            random_state=42,\n",
    "            n_jobs=3\n",
    "            ),\n",
    "        param_distributions=log_params,\n",
    "        n_jobs=3,\n",
    "        n_iter=3\n",
    "        )\n",
    ")\n",
    "\n",
    "# fit the model with training data\n",
    "model_log.fit(X_train, y_train)\n",
    "\n",
    "# get accuracy score for the fit model, for both train and val data\n",
    "train_score = accuracy_score(y_train, model_log.predict(X_train))\n",
    "val_score = accuracy_score(y_val, model_log.predict(X_val))\n",
    "\n",
    "# display the results\n",
    "print('\\n###################################################################\\n')\n",
    "print('Logistic Regression Accuracy Scores:')\n",
    "print('Train Score:', train_score)\n",
    "print('Val Score:', val_score)\n",
    "print('\\n###################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DmgProne\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17:45:39] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "\n",
      "###################################################################\n",
      "\n",
      "XGBoost Accuracy Scores:\n",
      "Train Score: 0.9088210290525861\n",
      "Val Score: 0.9015342680178661\n",
      "\n",
      "###################################################################\n"
     ]
    }
   ],
   "source": [
    "# defining parameter distribution for RandomSearchCV\n",
    "boost_params = {\n",
    "    'max_depth': [6, 12, 18, 24],\n",
    "    'alpha': np.arange(0.0, 2.0, 0.1)\n",
    "}\n",
    "\n",
    "# establishing pipeline\n",
    "# randomize search is set to take only 3 samples to save on time, but \n",
    "# would ideally be increased to find better parameters\n",
    "model_boost = make_pipeline(\n",
    "    OrdinalEncoder(),\n",
    "    RandomizedSearchCV(\n",
    "        XGBClassifier(\n",
    "            random_state=42,\n",
    "            n_jobs=3\n",
    "            ),\n",
    "        param_distributions=boost_params,\n",
    "        n_jobs=3,\n",
    "        n_iter=3\n",
    "        )\n",
    ")\n",
    "\n",
    "# fit the model with training data\n",
    "model_boost.fit(X_train, y_train)\n",
    "\n",
    "# get accuracy score for the fit model, for both train and val data\n",
    "train_score = accuracy_score(y_train, model_boost.predict(X_train))\n",
    "val_score = accuracy_score(y_val, model_boost.predict(X_val))\n",
    "\n",
    "# display the results\n",
    "print('\\n###################################################################\\n')\n",
    "print('XGBoost Accuracy Scores:')\n",
    "print('Train Score:', train_score)\n",
    "print('Val Score:', val_score)\n",
    "print('\\n###################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Accuracy of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###################################################################\n",
      "\n",
      "Test Score: 0.9019069672042662\n",
      "\n",
      "###################################################################\n"
     ]
    }
   ],
   "source": [
    "# Getting test accuracy and reporting\n",
    "test_score = accuracy_score(y_test, model_boost.predict(X_test))\n",
    "\n",
    "print('\\n###################################################################\\n')\n",
    "print('Test Score:', test_score)\n",
    "print('\\n###################################################################')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "421eea92a488641f3e0a11853e8b00eb4b01935408da2fd194c7f1ec2b716661"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
